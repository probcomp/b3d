{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import trimesh\n",
    "import b3d\n",
    "import genjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import rerun as rr\n",
    "from PIL import Image\n",
    "from b3d import Pose\n",
    "from b3d import Mesh, Pose\n",
    "from b3d.pose import Pose, camera_from_position_and_target\n",
    "import h5py\n",
    "import b3d.utils as utils\n",
    "from scipy.spatial.transform import Rotation as scipyR\n",
    "# genjax.pretty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for reading physion metadata\n",
    "physion_assets_path = os.path.join(\n",
    "    b3d.get_root_path(),\n",
    "    \"assets/physion/\",)\n",
    "\n",
    "stim_name = 'pilot_it2_rollingSliding_simple_ramp_box_0024'\n",
    "\n",
    "hdf5_file_path = os.path.join(physion_assets_path,\n",
    "    f\"hdf5s/{stim_name}.hdf5\",\n",
    ")\n",
    "\n",
    "video_file_path = os.path.join(physion_assets_path,\n",
    "    f\"videos/{stim_name}_img.mp4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfov = 54.43222 \n",
    "near_plane = 0.1\n",
    "far_plane = 100\n",
    "depth_arr = []\n",
    "image_arr = []\n",
    "with h5py.File(hdf5_file_path, \"r\") as f:\n",
    "    # extract depth info\n",
    "    for key in f['frames'].keys():\n",
    "        depth = jnp.array(f['frames'][key]['images']['_depth_cam0'])\n",
    "        depth_arr.append(depth)\n",
    "        image = jnp.array(Image.open(io.BytesIO(f['frames'][key]['images']['_img_cam0'][:])))\n",
    "        image_arr.append(image)\n",
    "    depth_arr = jnp.asarray(depth_arr)\n",
    "    image_arr = jnp.asarray(image_arr)\n",
    "    FINAL_T, height, width = image_arr.shape[0], image_arr.shape[1], image_arr.shape[2]\n",
    "\n",
    "    # extract camera info\n",
    "    camera_azimuth = np.array(f['azimuth']['cam_0'])\n",
    "    camera_matrix = np.array(f['frames']['0000']['camera_matrices']['camera_matrix_cam0']).reshape((4, 4))\n",
    "    projection_matrix = np.array(f['frames']['0010']['camera_matrices']['projection_matrix_cam0']).reshape((4, 4))\n",
    "  \n",
    "    # Calculate the intrinsic matrix from vertical_fov.\n",
    "    # Motice that hfov and vfov are different if height != width\n",
    "    # We can also get the intrinsic matrix from opengl's perspective matrix.\n",
    "    # http://kgeorge.github.io/2014/03/08/calculating-opengl-perspective-matrix-from-opencv-intrinsic-matrix\n",
    "    vfov = vfov / 180.0 * np.pi\n",
    "    tan_half_vfov = np.tan(vfov / 2.0)\n",
    "    tan_half_hfov = tan_half_vfov * width / float(height)\n",
    "    fx = width / 2.0 / tan_half_hfov  # focal length in pixel space\n",
    "    fy = height / 2.0 / tan_half_vfov\n",
    "\n",
    "    # extract object info\n",
    "    object_ids = np.array(f['static']['object_ids'])\n",
    "    model_names = np.array(f['static']['model_names'])\n",
    "    distractors = np.array(f['static']['distractors']) if np.array(f['static']['distractors']).size != 0 else None\n",
    "    occluders = np.array(f['static']['occluders']) if np.array(f['static']['occluders']).size != 0 else None\n",
    "    initial_position = np.array(f['static']['initial_position'])\n",
    "    initial_rotation = np.array(f['static']['initial_rotation'])\n",
    "    scales = np.array(f['static']['scale'])\n",
    "    meshes_faces = [np.array(f['static']['mesh'][f'faces_{idx}']) for idx in range(len(object_ids))]\n",
    "    meshes_vertices = [np.array(f['static']['mesh'][f'vertices_{idx}']) for idx in range(len(object_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_mesh(vertices, scale_factor):\n",
    "    vertices[:, 0] *= scale_factor[0]\n",
    "    vertices[:, 1] *= scale_factor[1]\n",
    "    vertices[:, 2] *= scale_factor[2]\n",
    "    # vertices[:,[2,1]] = vertices[:,[1,2]]\n",
    "    return vertices\n",
    "\n",
    "def euler_angles_to_quaternion(euler: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert Euler angles to a quaternion.\n",
    "\n",
    "    Source: https://pastebin.com/riRLRvch\n",
    "\n",
    "    :param euler: The Euler angles vector.\n",
    "\n",
    "    :return: The quaternion representation of the Euler angles.\n",
    "    \"\"\"\n",
    "    pitch = np.radians(euler[0] * 0.5)\n",
    "    cp = np.cos(pitch)\n",
    "    sp = np.sin(pitch)\n",
    "\n",
    "    yaw = np.radians(euler[1] * 0.5)\n",
    "    cy = np.cos(yaw)\n",
    "    sy = np.sin(yaw)\n",
    "\n",
    "    roll = np.radians(euler[2] * 0.5)\n",
    "    cr = np.cos(roll)\n",
    "    sr = np.sin(roll)\n",
    "\n",
    "    x = sy * cp * sr + cy * sp * cr\n",
    "    y = sy * cp * cr - cy * sp * sr\n",
    "    z = cy * cp * sr - sy * sp * cr\n",
    "    w = cy * cp * cr + sy * sp * sr\n",
    "    # return np.array([x, -z, -y, w])\n",
    "    return np.array([x, y, z, w])\n",
    "\n",
    "# def swap_yz_axes_quaternion(q):\n",
    "#     \"\"\"\n",
    "#     Perform a transformation on a quaternion `q` to account for swapping the y and z axes.\n",
    "    \n",
    "#     Parameters:\n",
    "#         q (array-like): Quaternion [w, x, y, z] in reference frame A.\n",
    "\n",
    "#     Returns:\n",
    "#         q_transformed (array-like): Quaternion [w, x, y, z] in reference frame B.\n",
    "#     \"\"\"\n",
    "#     # Convert to [w, x, y, z] format\n",
    "#     q[[1,3]] = q[[3,1]]\n",
    "\n",
    "#     # Define the quaternion for a 90-degree rotation about the x-axis\n",
    "#     r_x = scipyR.from_euler('x', 90, degrees=True).as_quat()  # This gives [x, y, z, w] format\n",
    "\n",
    "#     # Convert to [w, x, y, z] format\n",
    "#     r_x = np.array([r_x[3], r_x[0], r_x[1], r_x[2]])\n",
    "    \n",
    "#     # Convert input quaternion q to [x, y, z, w] format\n",
    "#     q_input = np.array([q[1], q[2], q[3], q[0]])\n",
    "    \n",
    "#     # Convert to rotation objects\n",
    "#     q_rot = scipyR.from_quat(q_input)  # Create a rotation object from input quaternion\n",
    "#     r_rot = scipyR.from_quat(r_x)      # Create a rotation object for the swap rotation\n",
    "\n",
    "#     # Perform the transformation q' = r_x * q * r_x_inverse\n",
    "#     q_transformed = r_rot * q_rot * r_rot.inv()\n",
    "\n",
    "#     # Convert the result back to [w, x, y, z] format\n",
    "#     q_transformed = q_transformed.as_quat()\n",
    "#     return [q_transformed[0], q_transformed[1], q_transformed[2], q_transformed[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_model_ids = np.concatenate((np.where(model_names==distractors), np.where(model_names==occluders)), axis=0)\n",
    "included_model_names = [model_names[idx] for idx in range(len(object_ids)) if idx not in excluded_model_ids]\n",
    "included_model_ids = [object_ids[idx]-1 for idx in range(len(object_ids)) if idx not in excluded_model_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_initial_positions = [pos for idx, pos in enumerate(initial_position) if idx in included_model_ids]\n",
    "object_initial_rotations = [rot for idx, rot in enumerate(initial_rotation) if idx in included_model_ids]\n",
    "object_scales = [scale for idx, scale in enumerate(scales) if idx in included_model_ids]\n",
    "object_meshes = [(scale_mesh(vertex, object_scales[idx]), face) for idx, (face, vertex) in enumerate(zip(meshes_faces, meshes_vertices)) if idx in included_model_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3d.rr_init(\"demo_physion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rr.log(\"/\", rr.ViewCoordinates.RIGHT_HAND_Y_DOWN, static=True)  # Set an up-axis\n",
    "rr.log(\"/\", rr.ViewCoordinates.LEFT_HAND_Y_UP, static=True)  # Set an up-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose(position=Array([1.25, 0.  , 0.  ], dtype=float32), quaternion=Array([0., 0., 0., 1.], dtype=float32))\n",
      "Pose(position=Array([-0.625,  0.01 ,  0.   ], dtype=float32), quaternion=Array([0.000000e+00, 1.000000e+00, 0.000000e+00, 6.123234e-17], dtype=float32))\n",
      "Pose(position=Array([-0.8202851,  1.1506727,  0.       ], dtype=float32), quaternion=Array([ 0.04738792, -0.8796334 ,  0.4724209 ,  0.02859946], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "all_object_poses = []\n",
    "all_meshes = []\n",
    "for idx in range(len(included_model_ids)):\n",
    "    object_pose = Pose(jnp.asarray(object_initial_positions[idx]), jnp.asarray(euler_angles_to_quaternion(object_initial_rotations[idx])))\n",
    "    print(object_pose)\n",
    "    b3d.rr_log_pose(f\"{idx}\", object_pose)\n",
    "    all_object_poses.append(object_pose)\n",
    "\n",
    "    mesh = trimesh.Trimesh(vertices=object_meshes[idx][0], faces=object_meshes[idx][1])\n",
    "    mesh = b3d.Mesh.from_trimesh(mesh)\n",
    "    all_meshes.append(mesh)\n",
    "    mesh.transform(object_pose).rr_visualize(f\"mesh_{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9407364  2.983262  -1.6307147]\n"
     ]
    }
   ],
   "source": [
    "R = camera_matrix[:3,:3]\n",
    "T = camera_matrix[0:3, 3]\n",
    "a = np.array([-R[0,:], -R[1,:], -R[2,:]])\n",
    "b = np.array(T)\n",
    "camera_position_from_matrix = np.linalg.solve(a, b)\n",
    "# camera_position_from_matrix[[2,1]] = camera_position_from_matrix[[1,2]]\n",
    "print(camera_position_from_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_rotation_from_matrix = -np.transpose(R)\n",
    "camera_pose = Pose(\n",
    "    camera_position_from_matrix,\n",
    "    b3d.Rot.from_matrix(camera_rotation_from_matrix).as_quat()\n",
    ")\n",
    "utils.rr_log_pose(\"camera_pose\", camera_pose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera_look_at = jnp.array([0, 0, 0])\n",
    "# camera_pose = Pose.from_position_and_target(\n",
    "#     camera_position_from_matrix,\n",
    "#     camera_look_at,\n",
    "#     up=jnp.array([0.0, 1.0, 0.0]),\n",
    "# )\n",
    "# utils.rr_log_pose(\"camera_pose\", camera_pose)\n",
    "# # print(camera_pose.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlwang_ipe_genjax/b3d/.pixi/envs/gpu/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "renderer = b3d.RendererOriginal(\n",
    "    width=width,\n",
    "    height=height,\n",
    "    fx=fx,\n",
    "    fy=fy,\n",
    "    cx=width/2,\n",
    "    cy=height/2,\n",
    "    near=near_plane,\n",
    "    far=far_plane,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_mesh = Mesh.transform_and_merge_meshes(all_meshes, all_object_poses)\n",
    "scene_mesh_in_camera_frame = scene_mesh.transform(camera_pose.inv())\n",
    "# scene_mesh_in_camera_frame.rr_visualize(\"scene_mesh_in_camera_frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbd = renderer.render_rgbd_from_mesh(\n",
    "    scene_mesh_in_camera_frame\n",
    ")\n",
    "b3d.rr_log_depth(rgbd[...,3], \"depth/\")\n",
    "b3d.rr_log_depth(np.flip(depth_arr[0],1), \"depth/observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(hdf5_file_path, \"r\") as f:\n",
    "#     for key in f['frames'].keys():\n",
    "#         ang_vel = jnp.array(f['frames'][key]['objects']['rotations_cam0'])\n",
    "#         print(ang_vel)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
