{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import trimesh\n",
    "import b3d\n",
    "from jax.scipy.spatial.transform import Rotation as Rot\n",
    "from b3d import Pose\n",
    "import genjax\n",
    "import rerun as rr\n",
    "from tqdm import tqdm\n",
    "\n",
    "rr.init(\"demo.py\")\n",
    "rr.connect(\"127.0.0.1:8812\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load date\n",
    "path = os.path.join(b3d.get_root_path(),\n",
    "\"assets/shared_data_bucket/input_data/shout_on_desk.r3d.video_input.npz\")\n",
    "video_input = b3d.VideoInput.load(path)\n",
    "\n",
    "# Get intrinsics\n",
    "image_width, image_height, fx,fy, cx,cy,near,far = np.array(video_input.camera_intrinsics_depth)\n",
    "image_width, image_height = int(image_width), int(image_height)\n",
    "fx,fy, cx,cy,near,far = float(fx),float(fy), float(cx),float(cy),float(near),float(far)\n",
    "\n",
    "# Get RGBS and Depth\n",
    "rgbs = video_input.rgb[::3] / 255.0\n",
    "xyzs = video_input.xyz[::3]\n",
    "\n",
    "# Resize rgbs to be same size as depth.\n",
    "rgbs_resized = jnp.clip(jax.vmap(jax.image.resize, in_axes=(0, None, None))(\n",
    "    rgbs, (video_input.xyz.shape[1], video_input.xyz.shape[2], 3), \"linear\"\n",
    "), 0.0, 1.0)\n",
    "\n",
    "\n",
    "num_layers = 2048\n",
    "renderer = b3d.Renderer(image_width, image_height, fx, fy, cx, cy, near, far, num_layers)\n",
    "model = b3d.model_multiobject_gl_factory(renderer)\n",
    "importance_jit = jax.jit(model.importance)\n",
    "update_jit = jax.jit(model.update)\n",
    "\n",
    "# Arguments of the generative model.\n",
    "# These control the inlier / outlier decision boundary for color error and depth error.\n",
    "#color_error, depth_error = (jnp.float32(30.0), jnp.float32(0.02))\n",
    "color_error, depth_error = (jnp.float32(50.0), jnp.float32(0.02))\n",
    "# TODO: explain\n",
    "inlier_score, outlier_prob = (jnp.float32(5.0), jnp.float32(0.001))\n",
    "# TODO: explain\n",
    "color_multiplier, depth_multiplier = (jnp.float32(3000.0), jnp.float32(3000.0))\n",
    "\n",
    "\n",
    "# Defines the enumeration schedule.\n",
    "key = jax.random.PRNGKey(0)\n",
    "# Gridding on translation only.\n",
    "translation_deltas = Pose.concatenate_poses([jax.vmap(lambda p: Pose.from_translation(p))(jnp.stack(\n",
    "    jnp.meshgrid(\n",
    "        jnp.linspace(-0.01, 0.01, 11),\n",
    "        jnp.linspace(-0.01, 0.01, 11),\n",
    "        jnp.linspace(-0.01, 0.01, 11),\n",
    "    ),\n",
    "    axis=-1,\n",
    ").reshape(-1, 3)), Pose.identity()[None,...]])\n",
    "# Sample orientations from a VMF to define a \"grid\" over orientations.\n",
    "rotation_deltas = Pose.concatenate_poses([jax.vmap(Pose.sample_gaussian_vmf_pose, in_axes=(0,None, None, None))(\n",
    "    jax.random.split(jax.random.PRNGKey(0), 11*11*11),\n",
    "    Pose.identity(),\n",
    "    0.00001, 1000.0\n",
    "), Pose.identity()[None,...]])\n",
    "all_deltas =  Pose.stack_poses([translation_deltas, rotation_deltas])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Make empty library\n",
    "object_library = b3d.MeshLibrary.make_empty_library()\n",
    "\n",
    "# Take point cloud at frame 0\n",
    "point_cloud = jax.image.resize(xyzs[0], (xyzs[0].shape[0]//3, xyzs[0].shape[1]//3, 3), \"linear\").reshape(-1,3)\n",
    "colors = jax.image.resize(rgbs_resized[0], (xyzs[0].shape[0]//3, xyzs[0].shape[1]//3, 3), \"linear\").reshape(-1,3)\n",
    "\n",
    "# `make_mesh_from_point_cloud_and_resolution` takes a 3D positions, colors, and sizes of the boxes that we want\n",
    "# to place at each position and create a mesh\n",
    "vertices, faces, vertex_colors, face_colors = b3d.make_mesh_from_point_cloud_and_resolution(\n",
    "    point_cloud, colors,\n",
    "    point_cloud[:,2] / fx * 3.0 # This is scaling the size of the box to correspond to the effective size of the pixel in 3D. It really should be multiplied by 2.\n",
    "    # and the 6 makes it larger\n",
    ")\n",
    "object_library.add_object(vertices, faces, vertex_colors)\n",
    "\n",
    "\n",
    "# Initial trace for timestep 0\n",
    "START_T = 0\n",
    "trace, _ = importance_jit(\n",
    "    jax.random.PRNGKey(0),\n",
    "    genjax.choice_map(\n",
    "        dict([\n",
    "            (\"camera_pose\", Pose.identity()),\n",
    "            (\"object_pose_0\", Pose.identity()),\n",
    "            (\"object_0\", 0),\n",
    "            (\"observed_rgb_depth\", (rgbs_resized[START_T], xyzs[START_T,...,2])),\n",
    "        ])\n",
    "    ),\n",
    "    (jnp.arange(1),color_error,depth_error,inlier_score,outlier_prob,color_multiplier,depth_multiplier, object_library)\n",
    ")\n",
    "# Visualize trace\n",
    "#b3d.rerun_visualize_trace_t(trace, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:39<00:00,  3.06it/s]\n"
     ]
    }
   ],
   "source": [
    "ACQUISITION_T = 120\n",
    "for T_observed_image in tqdm(range(ACQUISITION_T)):\n",
    "    # Constrain on new RGB and Depth data.\n",
    "    trace = b3d.update_choices_jit(trace, key,\n",
    "        genjax.Pytree.const([\"observed_rgb_depth\"]),\n",
    "        (rgbs_resized[T_observed_image],xyzs[T_observed_image,...,2])\n",
    "    )\n",
    "    trace,key = b3d.enumerate_and_select_best_move(trace, genjax.Pytree.const([\"camera_pose\"]), key, all_deltas)\n",
    "    b3d.rerun_visualize_trace_t(trace, T_observed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_center = 185 #152\n",
    "y_center = 105 #88\n",
    "del_pix = 5 #15#10#5\n",
    "\n",
    "local_points = xyzs[ACQUISITION_T,x_center-del_pix:x_center+del_pix,y_center-del_pix:y_center+del_pix,:].reshape(-1,3)\n",
    "local_rgbs = rgbs_resized[ACQUISITION_T,x_center-del_pix:x_center+del_pix,y_center-del_pix:y_center+del_pix,:].reshape(-1,3)\n",
    "patch_center = xyzs[ACQUISITION_T,x_center,y_center,:]\n",
    "\n",
    "point_cloud = local_points\n",
    "point_cloud_colors = local_rgbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [01:21<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create new mesh.\n",
    "vertices, faces, vertex_colors, face_colors = b3d.make_mesh_from_point_cloud_and_resolution(\n",
    "    point_cloud, point_cloud_colors, point_cloud[:,2] / fx * 2.0\n",
    ")\n",
    "object_pose = Pose.from_translation(vertices.mean(0))\n",
    "vertices = object_pose.inverse().apply(vertices)\n",
    "object_library.add_object(vertices, faces, vertex_colors)\n",
    "\n",
    "single_object_trace = trace\n",
    "\n",
    "trace = single_object_trace\n",
    "\n",
    "trace, _ = importance_jit(\n",
    "    jax.random.PRNGKey(0),\n",
    "    genjax.choice_map(\n",
    "        dict([\n",
    "            (\"camera_pose\", trace[\"camera_pose\"]),\n",
    "            (\"object_pose_0\", trace[\"object_pose_0\"]),\n",
    "            (\"object_pose_1\", trace[\"camera_pose\"] @ object_pose),\n",
    "            (\"object_0\", 0),\n",
    "            (\"object_1\", 1),\n",
    "            (\"observed_rgb_depth\", (rgbs_resized[ACQUISITION_T], xyzs[ACQUISITION_T,...,2])),\n",
    "        ])\n",
    "    ),\n",
    "    (jnp.arange(2),color_error,depth_error,inlier_score,outlier_prob,color_multiplier,depth_multiplier, object_library)\n",
    ")\n",
    "# Visualize trace\n",
    "b3d.rerun_visualize_trace_t(trace, ACQUISITION_T)\n",
    "\n",
    "FINAL_T = len(xyzs)\n",
    "for T_observed_image in tqdm(range(ACQUISITION_T, FINAL_T)):\n",
    "    # Constrain on new RGB and Depth data.\n",
    "    trace = b3d.update_choices_jit(trace, key,\n",
    "        genjax.Pytree.const([\"observed_rgb_depth\"]),\n",
    "        (rgbs_resized[T_observed_image],xyzs[T_observed_image,...,2])\n",
    "    )\n",
    "    trace,key = b3d.enumerate_and_select_best_move(trace, genjax.Pytree.const([\"camera_pose\"]), key, all_deltas)\n",
    "    trace,key = b3d.enumerate_and_select_best_move(trace, genjax.Pytree.const([f\"object_pose_1\"]), key, all_deltas)\n",
    "    b3d.rerun_visualize_trace_t(trace, T_observed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
