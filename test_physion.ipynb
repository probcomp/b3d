{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import trimesh\n",
    "import b3d\n",
    "import genjax\n",
    "import jax\n",
    "import b3d.bayes3d as bayes3d\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import rerun as rr\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from genjax import Pytree\n",
    "# from genjax._src.core.serialization.msgpack import msgpack_serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_mesh(vertices, scale_factor):\n",
    "    vertices[:, 0] *= scale_factor[0]\n",
    "    vertices[:, 1] *= scale_factor[1]\n",
    "    vertices[:, 2] *= scale_factor[2]\n",
    "    return vertices\n",
    "\n",
    "def euler_angles_to_quaternion(euler: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert Euler angles to a quaternion.\n",
    "\n",
    "    Source: https://pastebin.com/riRLRvch\n",
    "\n",
    "    :param euler: The Euler angles vector.\n",
    "\n",
    "    :return: The quaternion representation of the Euler angles.\n",
    "    \"\"\"\n",
    "    pitch = np.radians(euler[0] * 0.5)\n",
    "    cp = np.cos(pitch)\n",
    "    sp = np.sin(pitch)\n",
    "\n",
    "    yaw = np.radians(euler[1] * 0.5)\n",
    "    cy = np.cos(yaw)\n",
    "    sy = np.sin(yaw)\n",
    "\n",
    "    roll = np.radians(euler[2] * 0.5)\n",
    "    cr = np.cos(roll)\n",
    "    sr = np.sin(roll)\n",
    "\n",
    "    x = sy * cp * sr + cy * sp * cr\n",
    "    y = sy * cp * cr - cy * sp * sr\n",
    "    z = cy * cp * sr - sy * sp * cr\n",
    "    w = cy * cp * cr + sy * sp * sr\n",
    "    return np.array([x, y, z, w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load basic information from hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for reading physion metadata\n",
    "physion_assets_path = os.path.join(\n",
    "    b3d.get_root_path(),\n",
    "    \"assets/physion/\",)\n",
    "\n",
    "# stim_name = 'lf_0/dominoes_all_movies/pilot_dominoes_0mid_d3chairs_o1plants_tdwroom_0012'\n",
    "stim_name = '0012_256'\n",
    "\n",
    "hdf5_file_path = os.path.join(physion_assets_path,\n",
    "    f\"{stim_name}.hdf5\",\n",
    ")\n",
    "\n",
    "mesh_file_path = os.path.join(physion_assets_path,\n",
    "    f\"all_flex_meshes/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfov = 54.43222 \n",
    "near_plane = 0.1\n",
    "far_plane = 100\n",
    "depth_arr = []\n",
    "image_arr = []\n",
    "with h5py.File(hdf5_file_path, \"r\") as f:\n",
    "    # extract depth info\n",
    "    for key in f['frames'].keys():\n",
    "        depth = jnp.array(f['frames'][key]['images']['_depth_cam0'])\n",
    "        depth_arr.append(depth)\n",
    "        image = jnp.array(Image.open(io.BytesIO(f['frames'][key]['images']['_img_cam0'][:])))\n",
    "        image_arr.append(image)\n",
    "    depth_arr = jnp.asarray(depth_arr)\n",
    "    image_arr = jnp.asarray(image_arr)/255\n",
    "    FINAL_T, height, width = image_arr.shape[0], image_arr.shape[1], image_arr.shape[2]\n",
    "\n",
    "    # extract camera info\n",
    "    camera_azimuth = np.array(f['azimuth']['cam_0'])\n",
    "    camera_matrix = np.array(f['frames']['0000']['camera_matrices']['camera_matrix_cam0']).reshape((4, 4))\n",
    "    projection_matrix = np.array(f['frames']['0010']['camera_matrices']['projection_matrix_cam0']).reshape((4, 4))\n",
    "  \n",
    "    # Calculate the intrinsic matrix from vertical_fov.\n",
    "    # Motice that hfov and vfov are different if height != width\n",
    "    # We can also get the intrinsic matrix from opengl's perspective matrix.\n",
    "    # http://kgeorge.github.io/2014/03/08/calculating-opengl-perspective-matrix-from-opencv-intrinsic-matrix\n",
    "    vfov = vfov / 180.0 * np.pi\n",
    "    tan_half_vfov = np.tan(vfov / 2.0)\n",
    "    tan_half_hfov = tan_half_vfov * width / float(height)\n",
    "    fx = width / 2.0 / tan_half_hfov  # focal length in pixel space\n",
    "    fy = height / 2.0 / tan_half_vfov\n",
    "\n",
    "    # extract object info\n",
    "    object_ids = np.array(f['static']['object_ids'])\n",
    "    model_names = np.array(f['static']['model_names'])\n",
    "    assert len(object_ids) == len(model_names)\n",
    "    distractors = np.array(f['static']['distractors']) if np.array(f['static']['distractors']).size != 0 else None\n",
    "    occluders = np.array(f['static']['occluders']) if np.array(f['static']['occluders']).size != 0 else None\n",
    "    initial_position = np.array(f['static']['initial_position'])\n",
    "    initial_rotation = np.array(f['static']['initial_rotation'])\n",
    "    scales = np.array(f['static']['scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_model_ids = np.concatenate((np.where(model_names==distractors), np.where(model_names==occluders)), axis=0)\n",
    "included_model_ids = [idx for idx in range(len(object_ids)) if idx not in excluded_model_ids]\n",
    "included_model_names = [model_names[idx] for idx in included_model_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_initial_positions = [pos for idx, pos in enumerate(initial_position) if idx in included_model_ids]\n",
    "object_initial_rotations = [rot for idx, rot in enumerate(initial_rotation) if idx in included_model_ids]\n",
    "object_scales = [scale for idx, scale in enumerate(scales) if idx in included_model_ids]\n",
    "object_meshes = []\n",
    "for idx, model_name in enumerate(included_model_names):\n",
    "    trim = trimesh.load(os.path.join(mesh_file_path, f\"{model_name.decode('UTF-8')}.obj\"))\n",
    "    object_meshes.append((scale_mesh(trim.vertices, object_scales[idx]), trim.faces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b3d modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.init(\"demo\")\n",
    "rr.connect(\"127.0.0.1:8812\")\n",
    "rr.log(\"/\", rr.ViewCoordinates.LEFT_HAND_Y_UP, static=True)  # Set an up-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_object_poses = []\n",
    "for idx in range(len(included_model_ids)):\n",
    "    object_pose = b3d.Pose(jnp.asarray(object_initial_positions[idx]), jnp.asarray(euler_angles_to_quaternion(object_initial_rotations[idx])))\n",
    "    all_object_poses.append(object_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 objects in library\n"
     ]
    }
   ],
   "source": [
    "# load original meshes without scaling\n",
    "object_library = bayes3d.MeshLibrary.make_empty_library()\n",
    "\n",
    "for obj in object_meshes:\n",
    "    vertex_colors = jnp.full(obj[0].shape, 0.4)\n",
    "    object_library.add_object(obj[0], obj[1], vertex_colors)\n",
    "\n",
    "print(f\"{object_library.get_num_objects()} objects in library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = camera_matrix[:3,:3]\n",
    "T = camera_matrix[0:3, 3]\n",
    "a = np.array([-R[0,:], -R[1,:], -R[2,:]])\n",
    "b = np.array(T)\n",
    "camera_position_from_matrix = np.linalg.solve(a, b)\n",
    "camera_rotation_from_matrix = -np.transpose(R)\n",
    "camera_pose = b3d.Pose(\n",
    "    camera_position_from_matrix,\n",
    "    b3d.Rot.from_matrix(camera_rotation_from_matrix).as_quat()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlwang_ipe_genjax/b3d/.pixi/envs/gpu/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defines the enumeration schedule.\n",
    "key = jax.random.PRNGKey(0)\n",
    "renderer = b3d.Renderer(\n",
    "    width=width,\n",
    "    height=height,\n",
    "    fx=fx,\n",
    "    fy=fy,\n",
    "    cx=width/2,\n",
    "    cy=height/2,\n",
    "    near=near_plane,\n",
    "    far=far_plane,\n",
    ")\n",
    "model = bayes3d.model_multiobject_gl_factory(renderer)\n",
    "importance_jit = jax.jit(model.importance)\n",
    "\n",
    "# Arguments of the generative model.\n",
    "# These control the inlier / outlier decision boundary for color error and depth error.\n",
    "color_error, depth_error = (1e100, 0.05)\n",
    "inlier_score, outlier_prob = (5.0, 0.00001)\n",
    "color_multiplier, depth_multiplier = (10000.0, 500.0)\n",
    "model_args = bayes3d.ModelArgs(\n",
    "    color_error,\n",
    "    depth_error,\n",
    "    inlier_score,\n",
    "    outlier_prob,\n",
    "    color_multiplier,\n",
    "    depth_multiplier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlwang_ipe_genjax/b3d/.pixi/envs/gpu/lib/python3.12/site-packages/jax/_src/abstract_arrays.py:68: RuntimeWarning: overflow encountered in cast\n",
      "  return canonical_concrete_aval(np.array(x, dtype=dtype), weak_type=weak_type)\n",
      "/home/hlwang_ipe_genjax/b3d/.pixi/envs/gpu/lib/python3.12/site-packages/jax/_src/interpreters/xla.py:158: RuntimeWarning: overflow encountered in cast\n",
      "  return np.asarray(\n"
     ]
    }
   ],
   "source": [
    "# Initial trace for timestep 0\n",
    "START_T = 0\n",
    "trace, _ = importance_jit(\n",
    "    jax.random.PRNGKey(0),\n",
    "    genjax.ChoiceMap.d(\n",
    "        dict(\n",
    "            [\n",
    "                (\"camera_pose\", camera_pose),\n",
    "                (\"object_pose_0\", all_object_poses[0]),\n",
    "                (\"object_pose_1\", all_object_poses[1]),\n",
    "                (\"object_pose_2\", all_object_poses[2]),\n",
    "                (\"object_0\", 0),\n",
    "                (\"object_1\", 1),\n",
    "                (\"object_2\", 2),\n",
    "                (\"observed_rgb_depth\",\n",
    "                    (jnp.flip(image_arr[START_T],1), jnp.flip(jnp.flip(depth_arr[START_T],0),1)),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    (jnp.arange(3), model_args, object_library),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes3d.rerun_visualize_trace_t(trace, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/hlwang_ipe_genjax/b3d/saved_traces/test.pickle\", \"wb\") as output_file:\n",
    "    msgpack_serialize.dump(trace, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/hlwang_ipe_genjax/b3d/saved_traces/test.pickle\", \"rb\") as output_file:\n",
    "    retrieved_tr = msgpack_serialize.load(output_file, model, (jnp.arange(3), model_args, object_library))\n",
    "# retrieved_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes3d.rerun_visualize_trace_t(retrieved_tr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridding on translation only.\n",
    "translation_deltas = b3d.Pose.concatenate_poses(\n",
    "    [\n",
    "        jax.vmap(lambda p: b3d.Pose.from_translation(p))(\n",
    "            jnp.stack(\n",
    "                jnp.meshgrid(\n",
    "                    jnp.linspace(-0.01, 0.01, 11),\n",
    "                    jnp.linspace(-0.01, 0.01, 11),\n",
    "                    jnp.linspace(-0.01, 0.01, 11),\n",
    "                ),\n",
    "                axis=-1,\n",
    "            ).reshape(-1, 3)\n",
    "        ),\n",
    "        b3d.Pose.identity()[None, ...],\n",
    "    ]\n",
    ")\n",
    "# Sample orientations from a VMF to define a \"grid\" over orientations.\n",
    "rotation_deltas = b3d.Pose.concatenate_poses(\n",
    "    [\n",
    "        jax.vmap(b3d.Pose.sample_gaussian_vmf_pose, in_axes=(0, None, None, None))(\n",
    "            jax.random.split(jax.random.PRNGKey(0), 11 * 11 * 11),\n",
    "            b3d.Pose.identity(),\n",
    "            0.00001,\n",
    "            1000.0,\n",
    "        ),\n",
    "        b3d.Pose.identity()[None, ...],\n",
    "    ]\n",
    ")\n",
    "all_deltas = b3d.Pose.stack_poses([translation_deltas, rotation_deltas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [02:33<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "FINAL_T = len(image_arr)\n",
    "for T_observed_image in tqdm(range(FINAL_T)):\n",
    "    # Constrain on new RGB and Depth data.\n",
    "    trace = b3d.update_choices(\n",
    "        trace,\n",
    "        Pytree.const((\"observed_rgb_depth\",)),\n",
    "        (jnp.flip(image_arr[T_observed_image],1), jnp.flip(jnp.flip(depth_arr[T_observed_image],0),1)),\n",
    "    )\n",
    "    # camera pose doesn't change in physion\n",
    "    # trace, key = bayes3d.enumerate_and_select_best_move(\n",
    "    #     trace, Pytree.const((\"camera_pose\",)), key, all_deltas\n",
    "    # )\n",
    "    trace, key = bayes3d.enumerate_and_select_best_move(\n",
    "        trace, Pytree.const((\"object_pose_0\",)), key, all_deltas\n",
    "    )\n",
    "    trace, key = bayes3d.enumerate_and_select_best_move(\n",
    "        trace, Pytree.const((\"object_pose_1\",)), key, all_deltas\n",
    "    )\n",
    "    trace, key = bayes3d.enumerate_and_select_best_move(\n",
    "        trace, Pytree.const((\"object_pose_2\",)), key, all_deltas\n",
    "    )\n",
    "    bayes3d.rerun_visualize_trace_t(trace, T_observed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
